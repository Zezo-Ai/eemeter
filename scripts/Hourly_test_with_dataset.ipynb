{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "from pathlib import Path\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from eemeter.development.data import HourlyData\n",
    "from itertools import combinations\n",
    "# import time\n",
    "\n",
    "# from hourly_test_utils import *\n",
    "from applied_data_science.bigquery.data import Meter_Data\n",
    "from eemeter import eemeter as em\n",
    "# from eemeter.common.metrics import BaselineTestingMetrics as Metrics\n",
    "from scipy.interpolate import RBFInterpolator\n",
    "\n",
    "# import multiprocessing as mp\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: /app/.recurve_cache/data/MCE/MCE_covid_solar_meta_1.pkl\n",
      "Done in 0.005 s\n",
      "Loading: /app/.recurve_cache/data/MCE/MCE_covid_solar_hourly_meter_1.pkl\n",
      "Done in 4.756 s\n"
     ]
    }
   ],
   "source": [
    "#load subsamples from the main MCE dataset\n",
    "dataset = 'mce'\n",
    "subsample = 1\n",
    "has_solar = True\n",
    "\n",
    "cache_dir = Path(\"/app/.recurve_cache/data\").resolve()\n",
    "\n",
    "if 'data' in globals():\n",
    "    del data\n",
    "\n",
    "data = Meter_Data(dataset, subsample, \"hourly\", solar=has_solar, cache_dir=cache_dir)\n",
    "meta = data.df['meta']\n",
    "subsample_df = data.df['meter']\n",
    "ids = subsample_df.index.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_opt = 0.15\n",
    "l1_ratio_opt = 0.05\n",
    "settings = em.HourlySettings(\n",
    "    TRAIN_FEATURES=['ghi'],\n",
    "    LAGGED_FEATURES=None, # 'ghi'\n",
    "    WINDOW=None,\n",
    "    SUPPLEMENTAL_DATA=None,\n",
    "    # SUPPLEMENTAL_DATA=['solar_supp'],\n",
    "    ALPHA=alpha_opt,\n",
    "    L1_RATIO=l1_ratio_opt,\n",
    "    SELECTION=\"cyclic\",\n",
    "    MAX_ITER=1000,\n",
    "    SEED=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get one meter\n",
    "meter_id = ids[758]\n",
    "meter = subsample_df.loc[meter_id].copy()\n",
    "# PV_interventation = \"2019-11-09\"\n",
    "PV_interventation = \"2021-01-21\"\n",
    "\n",
    "#change PV_interventation to datetime\n",
    "PV_interventation = pd.to_datetime(PV_interventation)\n",
    "meter['solar_supp'] = 0\n",
    "meter.loc[meter['date'] >= PV_interventation.date(), 'solar_supp'] = 1\n",
    "\n",
    "# select baseline period\n",
    "meter_baseline = meter.loc[meter['period'] == 'baseline'].copy()\n",
    "\n",
    "meta_meter = meta.loc[meter_id]\n",
    "\n",
    "# get unique pair of station_latitude and station_longitude\n",
    "unique_pairs = meta_meter[['station_latitude', 'station_longitude']].drop_duplicates()\n",
    "\n",
    "# print unique pairs\n",
    "METADATA = {\n",
    "    'station_latitude': unique_pairs['station_latitude'].values[0],\n",
    "    'station_longitude': unique_pairs['station_longitude'].values[0],\n",
    "}\n",
    "\n",
    "#TODO: keep it simple and let the input force the output\n",
    "OUTPUT_FEATURES=['temperature', 'observed', 'model']\n",
    "if 'solar_supp' in settings.TRAIN_FEATURES:\n",
    "    OUTPUT_FEATURES.append('solar_supp')\n",
    "if 'ghi' in settings.TRAIN_FEATURES:\n",
    "    CONSIDER_SOLAR = True\n",
    "    OUTPUT_FEATURES.append('ghi')\n",
    "    OUTPUT_FEATURES.append('clearsky_ghi')\n",
    "else:\n",
    "    CONSIDER_SOLAR = False\n",
    "kwargs = {\n",
    "    'solar': CONSIDER_SOLAR,\n",
    "    'metadata': METADATA,\n",
    "    'outputs': OUTPUT_FEATURES,\n",
    "}\n",
    "data_baseline = HourlyData(meter_baseline, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "temperature     128\n",
       "observed       1862\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meter_baseline[['temperature', 'observed']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping 16,   None-0880849278_1-0880849278_1 due to empty baseline dataSkipping 32,   None-1396433705_1-1396433705_1 due to empty baseline data\n",
      "Skipping 232,   None-1771940710_1-1771940710_1 due to empty baseline dataSkipping 240,   None-1782589105_1-1782589105_1 due to empty baseline data\n",
      "Skipping 88,   None-1545680582_1-1545680582_1 due to empty baseline dataSkipping 224,   None-1761946610_1-1761946610_1 due to empty baseline data\n",
      "\n",
      "\n",
      "\n",
      "Skipping 241,   None-1783689710_1-1783689710_1 due to empty baseline data\n",
      "Skipping 57,   None-1474037010_1-1474037010_1 due to empty baseline data\n",
      "Skipping 193,   None-1718143205_1-1718143205_1 due to empty baseline dataSkipping 73,   None-1509752510_1-1509752510_1 due to empty baseline data\n",
      "\n",
      "Skipping 74,   None-1510867905_1-1510867905_1 due to empty baseline data\n",
      "Skipping 82,   None-1530517010_1-1530517010_1 due to empty baseline data\n",
      "Skipping 83,   None-1530649410_1-1530649410_1 due to empty baseline data\n",
      "Skipping 84,   None-1535257901_1-1535257901_1 due to empty baseline data\n",
      "Skipping 43,   None-1439090405_1-1439090405_1 due to empty baseline data\n",
      "Skipping 44,   None-1442508410_1-1442508410_1 due to empty baseline data\n",
      "Skipping 67,   None-1494979905_1-1494979905_1 due to empty baseline data\n",
      "Skipping 68,   None-1499375405_1-1499375405_1 due to empty baseline data\n",
      "Skipping 236,   None-1778283210_1-1778283210_1 due to empty baseline data\n",
      "Skipping 179,   None-1699025105_1-1699025105_1 due to empty baseline data\n",
      "Skipping 180,   None-1700100805_1-1700100805_1 due to empty baseline data\n",
      "Skipping 20,   None-1136854173_1-1136854173_1 due to empty baseline data\n",
      "Skipping 27,   None-1364423102_1-1364423102_1 due to empty baseline data\n",
      "Skipping 28,   None-1374597705_1-1374597705_1 due to empty baseline data\n",
      "Skipping 46,   None-1449502905_1-1449502905_1 due to empty baseline data\n",
      "Skipping 107,   None-1582571110_1-1582571110_1 due to empty baseline data\n",
      "Skipping 37,   None-1417857535_1-1417857535_1 due to empty baseline data\n",
      "Skipping 148,   None-1652012410_1-1652012410_1 due to empty baseline data\n",
      "Skipping 182,   None-1701965210_1-1701965210_1 due to empty baseline data\n",
      "Skipping 229,   None-1767916610_1-1767916610_1 due to empty baseline data\n",
      "Skipping 197,   None-1724402205_1-1724402205_1 due to empty baseline data\n",
      " 161,   None-1672493110_1-1672493110_1 has {'observed': 5613} missing data\n",
      "Skipping 39,   None-1426888605_1-1426888605_1 due to empty baseline data\n",
      " 2,   None-0083358324_1-0083358324_1 has {'observed': 4985} missing data\n",
      "Skipping 3,   None-0146007517_1-0146007517_1 due to empty baseline data\n",
      "Skipping 247,   None-1790866905_1-1790866905_1 due to empty baseline data\n",
      "Skipping 23,   None-1289535580_1-1289535580_1 due to empty baseline data\n",
      " 114,   None-1590056523_1-1590056523_1 has {'observed': 6281} missing data\n",
      "Skipping 156,   None-1666248810_1-1666248810_1 due to empty baseline data\n",
      "Skipping 150,   None-1655979105_1-1655979105_1 due to empty baseline data\n",
      "Skipping 151,   None-1657557810_1-1657557810_1 due to empty baseline data\n",
      "Skipping 272,   None-1831785405_1-1831785405_1 due to empty baseline data\n",
      "Skipping 62,   None-1490176410_1-1490176410_1 due to empty baseline data\n",
      "Skipping 63,   None-1490263005_1-1490263005_1 due to empty baseline data\n",
      "Skipping 142,   None-1645115905_1-1645115905_1 due to empty baseline data\n",
      "Skipping 214,   None-1748286310_1-1748286310_1 due to empty baseline data\n",
      "Skipping 116,   None-1595311205_1-1595311205_1 due to empty baseline data\n",
      "Skipping 289,   None-1861281605_1-1861281605_1 due to empty baseline data\n",
      " 47,   None-1449825410_1-1449825410_1 has {'observed': 6089} missing data\n",
      "Skipping 328,   None-1925967405_1-1925967405_1 due to empty baseline data\n",
      "Skipping 190,   None-1711586905_1-1711586905_1 due to empty baseline data\n",
      "Skipping 250,   None-1797559805_1-1797559805_1 due to empty baseline data\n",
      " 215,   None-1749655110_1-1749655110_1 has {'observed': 2921} missing data\n",
      "Skipping 298,   None-1873465605_1-1873465605_1 due to empty baseline data\n",
      "Skipping 299,   None-1873974805_1-1873974805_1 due to empty baseline data\n",
      " 164,   None-1676300210_1-1676300210_1 has {'observed': 1672} missing data\n",
      "Skipping 283,   None-1852922405_1-1852922405_1 due to empty baseline data\n",
      "Skipping 277,   None-1842127005_1-1842127005_1 due to empty baseline data\n",
      "Skipping 278,   None-1844526652_1-1844526652_1 due to empty baseline data\n",
      "Skipping 292,   None-1863976505_1-1863976505_1 due to empty baseline data\n",
      "Skipping 385,   None-2002260305_1-2002260305_1 due to empty baseline data\n",
      " 5,   None-0348885125_1-0348885125_1 has {'observed': 4697} missing data\n",
      "Skipping 339,   None-1943646705_1-1943646705_1 due to empty baseline data\n",
      " 111,   None-1588112305_1-1588112305_1 has {'observed': 4194} missing data\n",
      "Skipping 309,   None-1892415805_1-1892415805_1 due to empty baseline data\n",
      "Skipping 418,   None-2076163789_1-2076163789_1 due to empty baseline data\n",
      "Skipping 371,   None-1978572690_1-1978572690_1 due to empty baseline data\n",
      "Skipping 464,   None-2418147510_1-2418147510_1 due to empty baseline data\n",
      " 345,   None-1948557710_1-1948557710_1 has {'observed': 3865} missing data\n",
      "Skipping 480,   None-2573588905_1-2573588905_1 due to empty baseline data\n",
      " 279,   None-1845172505_1-1845172505_1 has {'observed': 5705} missing data\n",
      " 260,   None-1813200505_1-1813200505_1 has {'observed': 3366} missing data\n",
      " 333,   None-1934475805_1-1934475805_1 has {'observed': 1125} missing data\n",
      "Skipping 334,   None-1935105905_1-1935105905_1 due to empty baseline data\n",
      "Skipping 427,   None-2093363008_1-2093363008_1 due to empty baseline data\n",
      "Skipping 365,   None-1971955510_1-1971955510_1 due to empty baseline data\n",
      " 341,   None-1944215310_1-1944215310_1 has {'observed': 3737} missing data\n",
      "Skipping 397,   None-2029630105_1-2029630105_1 due to empty baseline data\n",
      "Skipping 343,   None-1945465205_1-1945465205_1 due to empty baseline data\n",
      "Skipping 474,   None-2522216210_1-2522216210_1 due to empty baseline data\n",
      " 441,   None-2132922505_1-2132922505_1 has {'observed': 6425} missing data\n",
      "Skipping 442,   None-2142005805_1-2142005805_1 due to empty baseline data\n",
      "Skipping 399,   None-2032299005_1-2032299005_1 due to empty baseline data\n",
      "Skipping 375,   None-1985451110_1-1985451110_1 due to empty baseline data\n",
      "Skipping 452,   None-2173299605_1-2173299605_1 due to empty baseline data\n",
      " 520,   None-2858989310_1-2858989310_1 has {'observed': 1265} missing data\n",
      "Skipping 592,   None-3205991305_1-3205991305_1 due to empty baseline data\n",
      "Skipping 507,   None-2783660405_1-2783660405_1 due to empty baseline data\n",
      "Skipping 508,   None-2792648805_1-2792648805_1 due to empty baseline data\n",
      " 326,   None-1923383005_1-1923383005_1 has {'observed': 1841} missing data\n",
      " 497,   None-2711121510_1-2711121510_1 has {'observed': 5157} missing data\n",
      "Skipping 498,   None-2714413305_1-2714413305_1 due to empty baseline data\n",
      "Skipping 569,   None-3121561505_1-3121561505_1 due to empty baseline data\n",
      "Skipping 485,   None-2621113466_1-2621113466_1 due to empty baseline data\n",
      " 381,   None-1997087410_1-1997087410_1 has {'observed': 4865} missing data\n",
      "Skipping 562,   None-3092998105_1-3092998105_1 due to empty baseline data\n",
      " 318,   None-1906363200_1-1906363200_1 has {'observed': 6617} missing data\n",
      "Skipping 476,   None-2552588705_1-2552588705_1 due to empty baseline data\n",
      "Skipping 539,   None-2959429205_1-2959429205_1 due to empty baseline data\n",
      "Skipping 540,   None-2963168505_1-2963168505_1 due to empty baseline data\n",
      " 437,   None-2121368505_1-2121368505_1 has {'observed': 2250} missing data\n",
      "Skipping 438,   None-2123043105_1-2123043105_1 due to empty baseline data\n",
      "Skipping 571,   None-3130853405_1-3130853405_1 due to empty baseline data\n",
      " 460,   None-2247947105_1-2247947105_1 has {'observed': 4049} missing data\n",
      " 563,   None-3093352910_1-3093352910_1 has {'observed': 1074} missing data\n",
      "Skipping 564,   None-3100804305_1-3100804305_1 due to empty baseline data\n",
      "Skipping 625,   None-3362461505_1-3362461505_1 due to empty baseline data\n",
      "Skipping 633,   None-3393779310_1-3393779310_1 due to empty baseline data\n",
      "Skipping 634,   None-3394641105_1-3394641105_1 due to empty baseline data\n",
      " 469,   None-2480724705_1-2480724705_1 has {'observed': 4745} missing data\n",
      "Skipping 462,   None-2370162994_1-2370162994_1 due to empty baseline data\n",
      "Skipping 543,   None-2973128205_1-2973128205_1 due to empty baseline data\n",
      "Skipping 557,   None-3068654005_1-3068654005_1 due to empty baseline data\n",
      "Skipping 525,   None-2890421905_1-2890421905_1 due to empty baseline data\n",
      "Skipping 526,   None-2900809305_1-2900809305_1 due to empty baseline data\n",
      " 454,   None-2185468010_1-2185468010_1 has {'observed': 6089} missing data\n",
      "Skipping 696,   None-3636157705_1-3636157705_1 due to empty baseline data\n",
      "Skipping 597,   None-3232351305_1-3232351305_1 due to empty baseline data\n",
      "Skipping 598,   None-3238066605_1-3238066605_1 due to empty baseline data\n",
      " 486,   None-2627903905_1-2627903905_1 has {'observed': 6641} missing data\n",
      "Skipping 704,   None-3661918805_1-3661918805_1 due to empty baseline data\n",
      "Skipping 471,   None-2484950079_1-2484950079_1 due to empty baseline data\n",
      "Skipping 588,   None-3193336710_1-3193336710_1 due to empty baseline data\n",
      "Skipping 589,   None-3196452105_1-3196452105_1 due to empty baseline data\n",
      "Skipping 590,   None-3197269105_1-3197269105_1 due to empty baseline data\n",
      "Skipping 575,   None-3144304105_1-3144304105_1 due to empty baseline data\n",
      "Skipping 612,   None-3300474810_1-3300474810_1 due to empty baseline data\n",
      "Skipping 567,   None-3113348410_1-3113348410_1 due to empty baseline data\n",
      "Skipping 628,   None-3373838010_1-3373838010_1 due to empty baseline data\n",
      " 494,   None-2693160505_1-2693160505_1 has {'observed': 2611} missing data\n",
      "Skipping 637,   None-3405062005_1-3405062005_1 due to empty baseline data\n",
      "Skipping 503,   None-2752205005_1-2752205005_1 due to empty baseline data\n",
      "Skipping 713,   None-3692809806_1-3692809806_1 due to empty baseline data\n",
      "Skipping 674,   None-3572559110_1-3572559110_1 due to empty baseline data\n",
      " 517,   None-2846188605_1-2846188605_1 has {'observed': 6541} missing data\n",
      "Skipping 651,   None-3482267805_1-3482267805_1 due to empty baseline data\n",
      "Skipping 746,   None-3780090105_1-3780090105_1 due to empty baseline data\n",
      " 599,   None-3243058849_1-3243058849_1 has {'observed': 4385} missing data\n",
      "Skipping 785,   None-3893429805_1-3893429805_1 due to empty baseline data\n",
      "Skipping 621,   None-3343111805_1-3343111805_1 due to empty baseline data\n",
      "Skipping 793,   None-3911713205_1-3911713205_1 due to empty baseline data\n",
      "Skipping 606,   None-3279765610_1-3279765610_1 due to empty baseline data\n",
      "Skipping 716,   None-3703171205_1-3703171205_1 due to empty baseline data\n",
      "Skipping 777,   None-3872566205_1-3872566205_1 due to empty baseline data\n",
      "Skipping 739,   None-3756715205_1-3756715205_1 due to empty baseline data\n",
      " 675,   None-3576041810_1-3576041810_1 has {'observed': 6305} missing data\n",
      "Skipping 856,   None-4381572305_1-4381572305_1 due to empty baseline data\n",
      "Skipping 802,   None-3933859005_1-3933859005_1 due to empty baseline data\n",
      " 747,   None-3782324905_1-3782324905_1 has {'observed': 4243} missing data\n",
      " 644,   None-3438945305_1-3438945305_1 has {'observed': 1294} missing data\n",
      "Skipping 764,   None-3829543210_1-3829543210_1 due to empty baseline data\n",
      "Skipping 670,   None-3562113305_1-3562113305_1 due to empty baseline data\n",
      "Skipping 804,   None-3944943505_1-3944943505_1 due to empty baseline data\n",
      "Skipping 805,   None-3955486305_1-3955486305_1 due to empty baseline data\n",
      " 848,   None-4336637805_1-4336637805_1 has {'observed': 1580} missing data\n",
      "Skipping 888,   None-4646867510_1-4646867510_1 due to empty baseline data\n",
      "Skipping 827,   None-4240304505_1-4240304505_1 due to empty baseline data\n",
      "Skipping 828,   None-4256348905_1-4256348905_1 due to empty baseline data\n",
      "Skipping 807,   None-3965844610_1-3965844610_1 due to empty baseline data\n",
      " 880,   None-4561863805_1-4561863805_1 has {'observed': 1442} missing data\n",
      " 873,   None-4496344405_1-4496344405_1 has {'observed': 1598} missing data\n",
      "Skipping 874,   None-4498898510_1-4498898510_1 due to empty baseline data\n",
      " 661,   None-3541360205_1-3541360205_1 has {'observed': 6428} missing data\n",
      "Skipping 836,   None-4290077910_1-4290077910_1 due to empty baseline data\n",
      "Skipping 812,   None-3985701498_1-3985701498_1 due to empty baseline data\n",
      "Skipping 757,   None-3810431443_1-3810431443_1 due to empty baseline data\n",
      " 866,   None-4471885505_1-4471885505_1 has {'observed': 1477} missing data\n",
      " 842,   None-4316645405_1-4316645405_1 has {'observed': 1582} missing data\n",
      "Skipping 936,   None-7247964102_1-7247964102_1 due to empty baseline data\n",
      "Skipping 822,   None-4200705805_1-4200705805_1 due to empty baseline data\n",
      " 881,   None-4572108305_1-4572108305_1 has {'observed': 1442} missing data\n",
      "Skipping 882,   None-4573864310_1-4573864310_1 due to empty baseline data\n",
      "Skipping 883,   None-4594200005_1-4594200005_1 due to empty baseline data\n",
      "Skipping 884,   None-4594624605_1-4594624605_1 due to empty baseline data\n",
      "Skipping 976,   None-9420342301_1-9420342301_1 due to empty baseline data\n",
      " 912,   None-5718582055_1-5718582055_1 has {'observed': 2225} missing data\n",
      "Skipping 984,   None-9898886798_1-9898886798_1 due to empty baseline data\n",
      " 867,   None-4472014805_1-4472014805_1 has {'observed': 1610} missing data\n",
      " 813,   None-3996778405_1-3996778405_1 has {'observed': 1580} missing data\n",
      " 851,   None-4346437905_1-4346437905_1 has {'observed': 1443} missing data\n",
      "Skipping 852,   None-4346812505_1-4346812505_1 due to empty baseline data\n",
      "Skipping 853,   None-4346905305_1-4346905305_1 due to empty baseline data\n",
      "Skipping 893,   None-4751147905_1-4751147905_1 due to empty baseline data\n",
      "Skipping 938,   None-7386783925_1-7386783925_1 due to empty baseline data\n",
      "Skipping 939,   None-7439460505_1-7439460505_1 due to empty baseline data\n",
      "Skipping 775,   None-3863385905_1-3863385905_1 due to empty baseline data\n",
      " 885,   None-4596072705_1-4596072705_1 has {'observed': 1443} missing data\n",
      "Skipping 855,   None-4368506675_1-4368506675_1 due to empty baseline data\n",
      "Skipping 725,   None-3725929505_1-3725929505_1 due to empty baseline data\n",
      " 758,   None-3813489410_1-3813489410_1 has {'observed': 1861} missing data\n",
      "Skipping 946,   None-7887205005_1-7887205005_1 due to empty baseline data\n",
      " 877,   None-4541961694_1-4541961694_1 has {'observed': 3809} missing data\n",
      " 845,   None-4326880605_1-4326880605_1 has {'observed': 1467} missing data\n",
      " 931,   None-6895844147_1-6895844147_1 has {'observed': 2201} missing data\n",
      " 978,   None-9518178747_1-9518178747_1 has {'observed': 5009} missing data\n",
      " 870,   None-4482281305_1-4482281305_1 has {'observed': 1582} missing data\n",
      " 907,   None-5464858218_1-5464858218_1 has {'observed': 6545} missing data\n",
      " 847,   None-4336160005_1-4336160005_1 has {'observed': 1442} missing data\n",
      " 749,   None-3788030905_1-3788030905_1 has {'observed': 2175} missing data\n",
      "Skipping 950,   None-8136058409_1-8136058409_1 due to empty baseline data\n",
      "Skipping 918,   None-6090540376_1-6090540376_1 due to empty baseline data\n"
     ]
    }
   ],
   "source": [
    "#TODO: keep it simple and let the input force the output\n",
    "OUTPUT_FEATURES=['temperature', 'observed', 'model']\n",
    "if 'solar_supp' in settings.TRAIN_FEATURES:\n",
    "    OUTPUT_FEATURES.append('solar_supp')\n",
    "if 'ghi' in settings.TRAIN_FEATURES:\n",
    "    CONSIDER_SOLAR = True\n",
    "    OUTPUT_FEATURES.append('ghi')\n",
    "    OUTPUT_FEATURES.append('clearsky_ghi')\n",
    "else:\n",
    "    CONSIDER_SOLAR = False\n",
    "\n",
    "\n",
    "arglist = []\n",
    "for i, meter_id in enumerate(ids):\n",
    "    meta_meter = meta.loc[meter_id]\n",
    "    # get unique pair of station_latitude and station_longitude\n",
    "    unique_pairs = meta_meter[['station_latitude', 'station_longitude']].drop_duplicates()\n",
    "    try:\n",
    "        try:\n",
    "            METADATA = {\n",
    "            'station_latitude': unique_pairs['station_latitude'][0],\n",
    "            'station_longitude': unique_pairs['station_longitude'][0],\n",
    "            }\n",
    "        except:\n",
    "            METADATA = {\n",
    "            'station_latitude': unique_pairs['station_latitude'],\n",
    "            'station_longitude': unique_pairs['station_longitude'],\n",
    "            }\n",
    "    except:\n",
    "        try:\n",
    "            METADATA = {\n",
    "                'station_latitude': unique_pairs['station_latitude'].values[0],\n",
    "                'station_longitude': unique_pairs['station_longitude'].values[0],\n",
    "                }\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    kwargs = {\n",
    "    'solar': CONSIDER_SOLAR,\n",
    "    'metadata': METADATA,\n",
    "    'outputs': OUTPUT_FEATURES,\n",
    "    }\n",
    "    arglist.append((i, meter_id, kwargs))\n",
    "\n",
    "\n",
    "def check_df_decorator(arglist):\n",
    "    i, meter_id, kwargs = arglist\n",
    "    status = 'None'\n",
    "\n",
    "    details = [i, meter_id, status]\n",
    "\n",
    "    try:\n",
    "        meter = subsample_df.loc[meter_id].copy()\n",
    "        meter_baseline = meter.loc[meter['period'] == 'baseline'].copy()\n",
    "        # get unique pair of station_latitude and station_longitude\n",
    "\n",
    "        if meter_baseline.empty:\n",
    "            print(f\"Skipping {i},   {meter_id} due to empty baseline data\")\n",
    "            details[-1] = 'empty_baseline'\n",
    "            return details\n",
    "        data_baseline = HourlyData(meter_baseline, **kwargs)\n",
    "\n",
    "        if data_baseline.too_many_missing_data:\n",
    "            print(f\" {i},   {meter_id} has {data_baseline.missing_values_amount} missing data\")\n",
    "            details[-1] = 'too_many_missing_data'\n",
    "            \n",
    "        if data_baseline.df[data_baseline.to_be_interpolated_columns].isnull().values.any():\n",
    "            print(f\"Skipping {i},   {meter_id} due to missing data, because of interpolaiton failed\")\n",
    "            details[-1] = 'interpolation_failed'\n",
    "        \n",
    "        return details\n",
    "    except:\n",
    "        print(f\"Skipping {i},   {meter_id} due to unknown error\")\n",
    "        details[-1] = 'unknown_error'\n",
    "        return details\n",
    "\n",
    "# for i, meter_id, kwargs in arglist:\n",
    "#     status = check_df_decorator((i, meter_id, kwargs))\n",
    "#     if status:\n",
    "#         print(f\"Skipping {i},   {meter_id} due to {status}\")\n",
    "#         continue\n",
    "import multiprocessing as mp\n",
    "with mp.Pool(mp.cpu_count()-1) as pool:\n",
    "    results = pool.map(check_df_decorator, arglist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "status\n",
       "None                     785\n",
       "empty_baseline           149\n",
       "too_many_missing_data     52\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#value counts of results\n",
    "results = pd.DataFrame(results, columns=['i', 'meter_id', 'status'])\n",
    "results['status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i</th>\n",
       "      <th>meter_id</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [i, meter_id, status]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.loc[results['status'] == 'unknown_error']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, meter_id in enumerate(ids):\n",
    "    print(i, meter_id)\n",
    "    meter = subsample_df.loc[meter_id].copy()\n",
    "\n",
    "    # select baseline period\n",
    "    meter_baseline = meter.loc[meter['period'] == 'baseline'].copy()\n",
    "\n",
    "    meta_meter = meta.loc[meter_id]\n",
    "\n",
    "    # get unique pair of station_latitude and station_longitude\n",
    "    unique_pairs = meta_meter[['station_latitude', 'station_longitude']].drop_duplicates()\n",
    "\n",
    "    # print unique pairs\n",
    "    METADATA = {\n",
    "        'station_latitude': unique_pairs['station_latitude'].values[0],\n",
    "        'station_longitude': unique_pairs['station_longitude'].values[0],\n",
    "    }\n",
    "\n",
    "    #TODO: keep it simple and let the input force the output\n",
    "    OUTPUT_FEATURES=['temperature', 'observed', 'model']\n",
    "    if 'solar_supp' in settings.TRAIN_FEATURES:\n",
    "        OUTPUT_FEATURES.append('solar_supp')\n",
    "    if 'ghi' in settings.TRAIN_FEATURES:\n",
    "        CONSIDER_SOLAR = True\n",
    "        OUTPUT_FEATURES.append('ghi')\n",
    "        OUTPUT_FEATURES.append('clearsky_ghi')\n",
    "    else:\n",
    "        CONSIDER_SOLAR = False\n",
    "    kwargs = {\n",
    "        'solar': CONSIDER_SOLAR,\n",
    "        'metadata': METADATA,\n",
    "        'outputs': OUTPUT_FEATURES,\n",
    "    }\n",
    "    if meter_baseline.empty:\n",
    "        print(f\"Skipping {i},   {meter_id} due to empty baseline data\")\n",
    "        continue\n",
    "    data_baseline = HourlyData(meter_baseline, **kwargs)\n",
    "\n",
    "    if data_baseline.too_many_missing_data:\n",
    "        print(f\"Skipping {i},   {meter_id} due to {data_baseline.missing_values_amount} missing data\")\n",
    "    if data_baseline.df[data_baseline.to_be_interpolated_columns].isnull().values.any():\n",
    "        print(f\"Skipping {i},   {meter_id} due to missing data, because of interpolaiton failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_datetime(df):\n",
    "    #get all the columns with datetime type #TODO: check if this is the best way to do this\n",
    "    datetime_columns = df.select_dtypes(include=[np.datetime64]).columns\n",
    "    # check if datetime is in the columns\n",
    "    if \"datetime\" in df.columns:\n",
    "        pass\n",
    "    elif \"datetime\" in df.index.names:\n",
    "        df['datetime'] = df.index\n",
    "        df = df.reset_index(drop=True)\n",
    "    elif \"start_local\" in df.columns:\n",
    "        df['datetime'] = df['start_local']\n",
    "        df = df.drop(columns=[\"start_local\"])\n",
    "    elif len(datetime_columns) > 0:\n",
    "        df['datetime'] = df[datetime_columns[0]]\n",
    "        df = df.drop(columns=[datetime_columns[0]])\n",
    "    else:\n",
    "        raise ValueError(\"datetime column not found\")\n",
    "    \n",
    "    #reset index to ensure datetime is not the index\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def get_contiguous_datetime(df):\n",
    "    # get earliest datetime and latest datetime\n",
    "    # make earliest start at 0 and latest end at 23, this ensures full days\n",
    "    earliest_datetime = df[\"datetime\"].min().replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "    latest_datetime = df[\"datetime\"].max().replace(hour=23, minute=0, second=0, microsecond=0)\n",
    "\n",
    "    # create a new index with all the hours between the earliest and latest datetime\n",
    "    complete_dt = pd.date_range(start=earliest_datetime, end=latest_datetime, freq='H').to_frame(index=False, name=\"datetime\")\n",
    "\n",
    "    # merge meter data with complete_dt\n",
    "    df = complete_dt.merge(df, on=\"datetime\", how=\"left\")\n",
    "    df['date'] = df['datetime'].dt.date\n",
    "    df['hour_of_day'] = df['datetime'].dt.hour\n",
    "\n",
    "    return df\n",
    "\n",
    "def remove_duplicate_datetime(df):\n",
    "    if \"observed\" in df.columns:\n",
    "        # find duplicate datetime values and remove if nan\n",
    "        duplicate_dt_mask = df.duplicated(subset=\"datetime\", keep=False)\n",
    "        observed_nan_mask = df['observed'].isna()\n",
    "        df = df[~(duplicate_dt_mask & observed_nan_mask)]\n",
    "\n",
    "        # if duplicated and observed is not nan, keep the largest abs(value)\n",
    "        df[\"abs_observed\"] = df[\"observed\"].abs()\n",
    "        df = df.sort_values(by=[\"datetime\", \"abs_observed\"], ascending=[True, False])\n",
    "        df = df.drop_duplicates(subset=\"datetime\", keep=\"first\")\n",
    "        df = df.drop(columns=[\"abs_observed\"])\n",
    "\n",
    "    else:\n",
    "        # TODO what if there is no observed column? Could have dup datetime with different temperatures\n",
    "        df = df.drop_duplicates(subset=\"datetime\", keep=\"first\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove random time slots for temperature and make them nan\n",
    "#set a random seed\n",
    "meter_baseline = check_datetime(meter_baseline)\n",
    "print(meter_baseline.isna().sum())\n",
    "\n",
    "meter_baseline_nans = meter_baseline.copy()\n",
    "interpolation_columns = ['observed']\n",
    "random_seeds = [42, 43]\n",
    "\n",
    "save_nan_vals = {}\n",
    "for i, col in enumerate(interpolation_columns):\n",
    "    np.random.seed(random_seeds[i])\n",
    "    random_slots = np.random.choice(meter_baseline['datetime'], 1000, replace=False)\n",
    "    save_nan_vals[col] = meter_baseline_nans.loc[meter_baseline_nans['datetime'].isin(random_slots)][['datetime', col]]\n",
    "    meter_baseline_nans.loc[meter_baseline_nans['datetime'].isin(random_slots), col] = np.nan\n",
    "print(meter_baseline_nans['temperature'].isna().sum())\n",
    "print(meter_baseline_nans['observed'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(meter_baseline_nans.shape)\n",
    "meter_baseline_nans = check_datetime(meter_baseline_nans)\n",
    "meter_baseline_nans = get_contiguous_datetime(meter_baseline_nans)\n",
    "meter_baseline_nans = remove_duplicate_datetime(meter_baseline_nans)\n",
    "print(meter_baseline_nans.shape)\n",
    "print(meter_baseline_nans['temperature'].isna().sum())\n",
    "print(meter_baseline_nans['observed'].isna().sum())\n",
    "meter_baseline_nans.set_index('datetime', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_baseline = HourlyData(meter_baseline_nans, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_baseline.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'observed'\n",
    "plt.plot(data_baseline.df[col])\n",
    "# plot the interpolated values on red\n",
    "plt.plot(data_baseline.df.loc[data_baseline.df[f'interpolated_{col}']==True, col], 'ro')\n",
    "plt.show()\n",
    "print(data_baseline.df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_dt = save_nan_vals['observed'].dropna()\n",
    "interp_vals = data_baseline.df.loc[data_baseline.df.index.isin(na_dt['datetime']), 'observed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_dt.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp_vals.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(interp_vals.values, na_dt['observed'].values, color='red')\n",
    "plt.show()\n",
    "\n",
    "#get the rmse error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "rmse = mean_squared_error(na_dt['observed'].values, interp_vals.values, squared=False)\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.5 vs 2.7\n",
    "\n",
    "2.2 vs 2.3\n",
    "\n",
    "\n",
    "2.2   vs 2.35\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # combinations_list = []\n",
    "        # for r in range(0, len(columns) + 1):\n",
    "        #     combinations_list.extend(combinations(columns, r))\n",
    "        # combinations_list = sorted(combinations_list, key=len, reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Interpolator:\n",
    "#     def __init__(self, grid_lag_lead_days = [-7, -1, 1, 7]):  \n",
    "#         self.grid_lag_lead_days = grid_lag_lead_days\n",
    "#         self.interp = RBFInterpolator\n",
    "#         self.interpolated_values = {}\n",
    "\n",
    "#     def interpolate(self, df, columns=['temperature', 'observed']):\n",
    "#         self.df = df\n",
    "#         self.columns = columns\n",
    "#         for col in columns:\n",
    "#             self.df[f'interpolated_{col}'] = False\n",
    "#         # Main method to perform the interpolation\n",
    "#         for col in self.columns:\n",
    "#             self._col_interpolation(col)\n",
    "        \n",
    "#         self.df.set_index('datetime', inplace=True)\n",
    "#         #for those datetime that we still haven't interpolated (for the columns), we will interpolate them with pd.interpolate\n",
    "#         for col in self.columns:\n",
    "#             still_na_datetime = self.df.loc[self.df[col].isna()].index\n",
    "#             print(f\"Interpolating {len(still_na_datetime)} nans for {col}\")\n",
    "#             self.df[col] = self.df[col].interpolate(method='time')\n",
    "#             self.df.loc[self.df.index.isin(still_na_datetime), f'interpolated_{col}'] = True\n",
    "        \n",
    "#         return self.df\n",
    "        \n",
    "    \n",
    "#     def _col_interpolation(self, col):\n",
    "#         # Method to interpolate a single column\n",
    "#         column_df = self.df[['datetime', 'date', col]]\n",
    "#         # add lag and lead columns\n",
    "#         interp_helper_cols = []\n",
    "#         for shift in self.grid_lag_lead_days:\n",
    "#             column_df[f'{col}_day_({shift})'] = column_df[col].shift(shift*24)\n",
    "#             interp_helper_cols.append(f'{col}_day_({shift})')\n",
    "\n",
    "#         column_df['hour_of_day'] = column_df['datetime'].dt.hour\n",
    "#         #group by date\n",
    "#         column_df = column_df.groupby('date')\n",
    "#         # interpolate each day\n",
    "#         interpolated_datetime = pd.DataFrame([], columns=['datetime', col])\n",
    "#         abnormal_date = []\n",
    "#         for date, group in column_df:\n",
    "#             nans = group[group[col].isna()]\n",
    "\n",
    "#             #TODO: update the values based on the previous interpolated points\n",
    "\n",
    "#             if nans.empty:\n",
    "#                 continue\n",
    "\n",
    "#             group, interpolated_datetime = self._scenario_selector_interpolator(group, col, interp_helper_cols, interpolated_datetime)\n",
    "\n",
    "#         interpolated_datetime_local = interpolated_datetime.set_index('datetime')\n",
    "#         self.interpolated_values[col] = interpolated_datetime_local\n",
    "\n",
    "#         #replace nans with the interpolated values for the column\n",
    "#         self.df.loc[self.df['datetime'].isin(interpolated_datetime_local.index), col] = self.df['datetime'].map(interpolated_datetime_local[col])\n",
    "#         self.df[f'interpolated_{col}'].loc[self.df['datetime'].isin(interpolated_datetime_local.index)] = True\n",
    "\n",
    "#     def _scenario_selector_interpolator(self, group, col, interp_helper_cols, interpolated_datetime):\n",
    "        \n",
    "#         def joint_value_options(input_string):\n",
    "#             from itertools import product\n",
    "#             str_lst = [int(i) for i in input_string.split(\" \")]\n",
    "#             all_possible_options = [list(option) for option in product([0, 1], repeat=len(str_lst))]\n",
    "#             all_possible_options = sorted([option for option in all_possible_options if all(i <= j for i, j in zip(option, str_lst))], reverse=True)\n",
    "#             all_possible_options.pop()#drop all zeros option\n",
    "#             return all_possible_options\n",
    "        \n",
    "#         normal_datapoints = group[~group[col].isna()]\n",
    "#         nans = group[group[col].isna()]\n",
    "\n",
    "#         nans_dt = nans['datetime'].values\n",
    "#         normal_dt = normal_datapoints['datetime'].values\n",
    "\n",
    "#         helper_matrix = group.loc[group['datetime'].isin(nans_dt)]\n",
    "#         helper_matrix_mask = 1 - helper_matrix[interp_helper_cols].isna().astype(int) #0,1 mask for hashing the data\n",
    "\n",
    "#         columns = helper_matrix_mask.columns.tolist()\n",
    "\n",
    "#         helper_matrix_mask['pointer'] = helper_matrix_mask.apply(lambda row: ' '.join(row.astype(str)), axis=1) #string of columns that have value or not\n",
    "#         helper_matrix_mask['class'] = helper_matrix_mask[columns].sum(axis=1)# this will be used for sorting the best options\n",
    "#         helper_matrix_mask['datetime'] = helper_matrix['datetime']\n",
    " \n",
    "#         datetime_lists = helper_matrix_mask.groupby('pointer')['datetime'].apply(list).to_dict()\n",
    "#         nan_groups = helper_matrix_mask.groupby(['class', 'pointer']).size().reset_index(name='counts')\n",
    "#         nan_groups = nan_groups.sort_values(by=['class', 'counts'], ascending=[False, False])\n",
    "#         nan_groups[f'datetime_lists_nans'] = nan_groups['pointer'].map(datetime_lists)\n",
    "        \n",
    "#         has_value = group.loc[group['datetime'].isin(normal_dt)].reset_index()\n",
    "#         has_value_mask = np.array(1 - has_value[interp_helper_cols].isna().astype(int).values)\n",
    "\n",
    "#         #search through option of each nan group\n",
    "#         for p in nan_groups['pointer'].unique():\n",
    "#             row = nan_groups.loc[nan_groups['pointer']==p]\n",
    "#             if not row['class'].values==0:\n",
    "#                 nan_pointer_options = np.array(joint_value_options(row['pointer'].values[0]))\n",
    "#                 filter = nan_pointer_options.sum(axis=1)\n",
    "#                 lookup_matrix = np.dot(has_value_mask, nan_pointer_options.transpose())\n",
    "#                 eligibles = np.where(lookup_matrix < filter, 0, 1).sum(axis=0)\n",
    "#                 count = np.where(eligibles > filter, 1, 0) # check if we have enough points for RBFinterpolate\n",
    "#                 if 1 in count:# give us the most promissing\n",
    "#                     idx = list(count).index(1)\n",
    "#                     best_scenario = nan_pointer_options[idx]\n",
    "#                     #select the columns based on best scenario\n",
    "#                     selected_cols = [c for i, c in enumerate(interp_helper_cols) if best_scenario[i]==1]\n",
    "#                     selected_has_value_index = np.where(np.where(lookup_matrix < filter, 0, 1)[:,idx]==1)\n",
    "#                     #get the point and value to feed interpolator\n",
    "#                     interp_points = has_value.loc[has_value.index.isin(selected_has_value_index[0])][selected_cols]\n",
    "#                     # add small random noise to remove singular matrix for RBF\n",
    "#                     noise = np.random.standard_normal(size=interp_points.shape)\n",
    "#                     interp_points += noise\n",
    "#                     interp_values = has_value.loc[has_value.index.isin(selected_has_value_index[0])][col]\n",
    "\n",
    "#                     nan_points_datetime =  nan_groups.loc[nan_groups['pointer']==p][f'datetime_lists_nans'].values[0]\n",
    "#                     nan_points = nans.loc[nans['datetime'].isin(nan_points_datetime)][selected_cols]\n",
    "\n",
    "#                     pred = self._interpolate_day(interp_points, interp_values, nan_points)\n",
    "\n",
    "#                     interpolated_rows = list(zip(nan_points_datetime, pred))\n",
    "#                     interpolated_rows = pd.DataFrame(interpolated_rows, columns=['datetime', col])\n",
    "#                     interpolated_datetime = pd.concat([interpolated_datetime, interpolated_rows], ignore_index=True)\n",
    "\n",
    "#                     interpolated_rows_local = interpolated_rows.set_index('datetime')\n",
    "#                     #replace nans with the interpolated values for the group\n",
    "#                     group.loc[group['datetime'].isin(nan_points_datetime), col] = group['datetime'].map(interpolated_rows_local[col])\n",
    "        \n",
    "#         return group, interpolated_datetime\n",
    "\n",
    "\n",
    "#     def _interpolate_day(self, points, values, nans):\n",
    "#         # Method to interpolate a single day\n",
    "#         if len(values)==0:\n",
    "#             return np.nan\n",
    "        \n",
    "#         interp = self.interp(points, values)\n",
    "#         pred = interp(nans)\n",
    "#         return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import RBFInterpolator, griddata, LinearNDInterpolator, NearestNDInterpolator, CloughTocher2DInterpolator\n",
    "\n",
    "\"\"\"griddata: need an actual limit for the input such as temp (like a grid) which we don't want to have\n",
    "   LinearNDInterpolator\" if the data is out of bound based on the train data, it gives us nan :( which is crazy that it doesn't even extrapolate\n",
    "   NearestNDInterpolator: values are descrete and way off sometimes (we have the mean value of couple of classes)\n",
    "   CloughTocher2DInterpolator: same as LinearNDInterpolator\n",
    "\n",
    "\n",
    "   Some crazy interpolation is already happening in the MCE temperature (last week and the day before temperatures were exactly the same (reason we we getting singularity in interpolation))\n",
    "\"\"\"\n",
    "def remove_duplicate_datetime(df):\n",
    "    if \"observed\" in df.columns:\n",
    "        # find duplicate datetime values and remove if nan\n",
    "        duplicate_dt_mask = df.duplicated(subset=\"datetime\", keep=False)\n",
    "        observed_nan_mask = df['observed'].isna()\n",
    "        df = df[~(duplicate_dt_mask & observed_nan_mask)]\n",
    "\n",
    "        # if duplicated and observed is not nan, keep the largest abs(value)\n",
    "        df[\"abs_observed\"] = df[\"observed\"].abs()\n",
    "        df = df.sort_values(by=[\"datetime\", \"abs_observed\"], ascending=[True, False])\n",
    "        df = df.drop_duplicates(subset=\"datetime\", keep=\"first\")\n",
    "        df = df.drop(columns=[\"abs_observed\"])\n",
    "\n",
    "    else:\n",
    "        # TODO what if there is no observed column? Could have dup datetime with different temperatures\n",
    "        df = df.drop_duplicates(subset=\"datetime\", keep=\"first\")\n",
    "\n",
    "    return df\n",
    "\n",
    "grouped_meter_baseline = meter_baseline_nans.copy()\n",
    "grouped_meter_baseline['interpolated'] = False\n",
    "grouped_meter_baseline = grouped_meter_baseline.groupby('date')\n",
    "\n",
    "\n",
    "none_regular_dates = []\n",
    "daily_thr_nans = 0\n",
    "seeds = [0, 1]\n",
    "for date, group in grouped_meter_baseline:\n",
    "    group = remove_duplicate_datetime(group)\n",
    "    if (\n",
    "        group['temperature'].isna().sum() > daily_thr_nans\n",
    "        ) or (\n",
    "            group['observed'].isna().sum() > daily_thr_nans\n",
    "        ):\n",
    "\n",
    "        #remove any duplicate\n",
    "\n",
    "        interpolation_columns = ['temperature', 'observed']\n",
    "\n",
    "        #get previous week date and yesterday\n",
    "        yesterday = date - pd.DateOffset(day=1)\n",
    "        yesterday_df = meter_baseline.loc[meter_baseline['date']==yesterday.date()]\n",
    "        yesterday_df = remove_duplicate_datetime(yesterday_df)\n",
    "\n",
    "        last_week = date - pd.DateOffset(weeks=1)\n",
    "        last_week_df = meter_baseline.loc[meter_baseline['date']==last_week.date()]\n",
    "        last_week_df = remove_duplicate_datetime(last_week_df)\n",
    "\n",
    "        if ((yesterday_df.empty==True) and (last_week_df.empty==True)):\n",
    "            #make a linear interpolation from what we have\n",
    "            group[interpolation_columns] = group[interpolation_columns].interpolate()\n",
    "\n",
    "        else:\n",
    "            intepolation_helper_days = []\n",
    "            if yesterday_df.empty==False:\n",
    "                intepolation_helper_days.append(yesterday_df)\n",
    "            \n",
    "            if last_week_df.empty==False:\n",
    "                intepolation_helper_days.append(last_week_df)\n",
    "\n",
    "            helper_vectors_nans = []\n",
    "            helper_vectors_normal = []\n",
    "\n",
    "            for inter_col in interpolation_columns:\n",
    "                if group[inter_col].isna().sum()>0:\n",
    "                    # print(inter_col)\n",
    "                    for i, interp_df in enumerate(intepolation_helper_days):\n",
    "                \n",
    "                        # get index of those without temperature\n",
    "                        nan_hours = group.loc[group[inter_col].isna()]['hour_of_day'].values\n",
    "                        interp_df_nans = interp_df.loc[interp_df['hour_of_day'].isin(nan_hours)][inter_col].values\n",
    "\n",
    "                        #get any other hours that is not in nan_hours\n",
    "                        normal_hours = group.loc[~group[inter_col].isna()]['hour_of_day'].values\n",
    "                        interp_df_normal = interp_df.loc[interp_df['hour_of_day'].isin(normal_hours)][inter_col].values\n",
    "\n",
    "                        #singularity prevention\n",
    "                        np.random.seed = seeds[i]\n",
    "                        noise = np.random.normal(0, 1, len(interp_df_normal))\n",
    "                        interp_df_normal += noise\n",
    "\n",
    "                        helper_vectors_normal.append(interp_df_normal)\n",
    "                        helper_vectors_nans.append(interp_df_nans)\n",
    "                        \n",
    "                    if len(helper_vectors_normal) == 1:\n",
    "                        temp_vector_normal = helper_vectors_normal[0].reshape(-1,1)\n",
    "                        temp_vector_nans = helper_vectors_nans[0].reshape(-1,1)\n",
    "                    else:\n",
    "                        temp_vector_normal = list(zip(helper_vectors_normal[0], helper_vectors_normal[1]))\n",
    "                        temp_vector_nans = list(zip(helper_vectors_nans[0], helper_vectors_nans[1]))\n",
    "\n",
    "                        \n",
    "                    # target_vector = group.loc[~group[inter_col].isna()][inter_col].values\n",
    "                    target_vector = group.loc[group['hour_of_day'].isin(normal_hours)][inter_col].values\n",
    "                    if len(target_vector)==0:\n",
    "                        none_regular_dates.append(date)\n",
    "                        pass\n",
    "                    else:\n",
    "                        interp = RBFInterpolator(temp_vector_normal, target_vector)\n",
    "                        preds = interp(temp_vector_nans)\n",
    "                        group.loc[group['hour_of_day'].isin(nan_hours), inter_col] = preds\n",
    "\n",
    "                        x = group['hour_of_day'].values\n",
    "                        for j, interp_df in enumerate(intepolation_helper_days):\n",
    "                            lbl = 'yesterday'\n",
    "                            if j==1:\n",
    "                                lbl = 'last week'\n",
    "                            plt.plot(x,interp_df[inter_col], label = lbl)\n",
    "                        \n",
    "                        plt.plot(x,group[inter_col], label = 'today')\n",
    "                        #plot temp values for interpolated hours as red\n",
    "                        xp = group.loc[group['hour_of_day'].isin(nan_hours)]['hour_of_day'].values\n",
    "                        plt.scatter(xp, group.loc[group['hour_of_day'].isin(nan_hours), inter_col], marker='^', color='red',label='interpolated')\n",
    "                        print(date)\n",
    "                        print(inter_col)\n",
    "                        plt.legend()\n",
    "                        plt.show()\n",
    "\n",
    "print(none_regular_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meter_baseline_nans\n",
    "print(meter_baseline_nans['temperature'].isna().sum())\n",
    "print(meter_baseline_nans['observed'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meter_baseline_nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_vector_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(target_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap fro the model._model.coef_\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "model_coef = model._model.coef_\n",
    "# # assuming model_coef is a 1D array, we reshape it to 2D for the heatmap\n",
    "# model_coef_2d = model_coef.reshape(-1, 1)\n",
    "\n",
    "#fig size\n",
    "plt.rcParams['figure.figsize'] = [12, 4]\n",
    "sns.heatmap(model_coef)\n",
    "plt.show()\n",
    "# get the model coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 0\n",
    "w = -1\n",
    "e = s + w\n",
    "plt.plot(pred_baseline['observed'].iloc[s:e].values, label='observed')\n",
    "plt.plot(pred_baseline['model'].iloc[s:e].values, label='CT')\n",
    "plt.plot(pred_baseline['predicted'].iloc[s:e].values, label='new model')\n",
    "# plt.plot(-pred_baseline['ghi'].iloc[s:e].values*3, label='ghi')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meter_reporting = meter.loc[meter['period'] == 'reporting'].copy()\n",
    "data_reporting = HourlyData(meter_reporting, **kwargs)\n",
    "reporting_pred = model.predict(data_reporting)\n",
    "reporting_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 0\n",
    "w = -1\n",
    "e = s + w\n",
    "plt.plot(reporting_pred['observed'].iloc[s:e].values, label='observed')\n",
    "plt.plot(reporting_pred['model'].iloc[s:e].values, label='CT')\n",
    "plt.plot(reporting_pred['predicted'].iloc[s:e].values, label='new model')\n",
    "# plt.plot(-pred_baseline['ghi'].iloc[s:e].values*3, label='ghi')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred['temperature'].quantile(0.95)\n",
    "# find days with temperature above 95th percentile\n",
    "hot_hours = reporting_pred[reporting_pred['temperature'] > reporting_pred['temperature'].quantile(0.995)]\n",
    "unique_days = np.unique(hot_hours.index.date)\n",
    "print(unique_days.shape)\n",
    "\n",
    "reporting_pred['date'] = reporting_pred.index.date\n",
    " # get days that are in unique_days\n",
    "hot_days = reporting_pred[reporting_pred['date'].isin(unique_days)]\n",
    "hot_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [15, 5]\n",
    "s = 24*0\n",
    "w = -1\n",
    "e = s + w\n",
    "plt.plot(hot_days['observed'].iloc[s:e].values, label='observed')\n",
    "plt.plot(hot_days['model'].iloc[s:e].values, label='CT')\n",
    "plt.plot(hot_days['predicted'].iloc[s:e].values, label='predicted')\n",
    "plt.legend()\n",
    "\n",
    "# plt.plot(-hot_days['ghi'].iloc[s:e].values*7, label='ghi')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 24*0\n",
    "w = 24*7\n",
    "e = s + w\n",
    "plt.plot(reporting_pred['observed'].iloc[s:e].values, label='observed')\n",
    "plt.plot(reporting_pred['model'].iloc[s:e].values, label='CT')\n",
    "plt.plot(reporting_pred['predicted'].iloc[s:e].values, label='new model')\n",
    "# plt.plot(-pred_baseline['ghi'].iloc[s:e].values*3, label='ghi')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PNRMSE(y_true, y_pred, iqr):\n",
    "    return np.sqrt(np.mean((y_true - y_pred)**2))/iqr\n",
    "\n",
    "def CVRMSE(y_true, y_pred, mn):\n",
    "    return np.sqrt(np.mean((y_true - y_pred)**2))/mn\n",
    "\n",
    "def MBE(y_true, y_pred):\n",
    "    return np.mean(y_true - y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the season/hour_of_week plots\n",
    "# get the season and hour_of_week\n",
    "seasons = {'winter': [11, 12, 1, 2], 'shoulder': [3, 4, 5, 10], 'summer': [6, 7, 8, 9]}\n",
    "reporting_pred['season'] = reporting_pred.index.month\n",
    "reporting_pred['season'] = reporting_pred['season'].apply(lambda x: 'winter' if x in seasons['winter'] else ('shoulder' if x in seasons['shoulder'] else 'summer'))\n",
    "reporting_pred['season'] = reporting_pred['season'].astype('category')\n",
    "\n",
    "reporting_pred['hour_of_week'] = reporting_pred.index.dayofweek*24 + reporting_pred.index.hour\n",
    "reporting_pred['hour_of_week'] = reporting_pred['hour_of_week'].astype('category')\n",
    "\n",
    "reporting_pred['hour'] = reporting_pred.index.hour\n",
    "reporting_pred['hour'] = reporting_pred['hour'].astype('category')\n",
    "\n",
    "\n",
    "# get the season/hour_of_week plots\n",
    "plt.rcParams['figure.figsize'] = [15, 5]\n",
    "\n",
    "plt.plot(reporting_pred.groupby(['season', 'hour_of_week'])['observed'].mean().values, label='observed')\n",
    "plt.plot(reporting_pred.groupby(['season', 'hour_of_week'])['model'].mean().values, label='CT')\n",
    "plt.plot(reporting_pred.groupby(['season', 'hour_of_week'])['predicted'].mean().values, label='predicted')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(reporting_pred.groupby(['season', 'hour'])['observed'].mean().values, label='observed')\n",
    "plt.plot(reporting_pred.groupby(['season', 'hour'])['model'].mean().values, label='CT')\n",
    "plt.plot(reporting_pred.groupby(['season', 'hour'])['predicted'].mean().values, label='predicted')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft = reporting_pred.copy()\n",
    "# dft = pred_baseline.copy()\n",
    "\n",
    "seasons = {'winter': [11, 12, 1, 2], 'shoulder': [3, 4, 5, 10], 'summer': [6, 7, 8, 9]}\n",
    "dft['season'] = dft.index.month\n",
    "dft['season'] = dft['season'].apply(lambda x: 'winter' if x in seasons['winter'] else ('shoulder' if x in seasons['shoulder'] else 'summer'))\n",
    "dft['season'] = dft['season'].astype('category')\n",
    "\n",
    "dft['hour_of_week'] = dft.index.dayofweek*24 + dft.index.hour\n",
    "dft['hour_of_week'] = dft['hour_of_week'].astype('category')\n",
    "\n",
    "dft['hour'] = dft.index.hour\n",
    "dft['hour'] = dft['hour'].astype('category')\n",
    "\n",
    "# plot two weeks of data for oeem , new_model and observed\n",
    "start = 24*0\n",
    "pltwindow = 24*7\n",
    "end = start + pltwindow\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (21,6)\n",
    "# higher dpi for better quality\n",
    "plt.rcParams['figure.dpi'] = 500\n",
    "#x axis as hours\n",
    "# plt.plot(dft['start_local'][start:end], label='start_local', linestyle='--')\n",
    "# plt.plot(dft['observed'][start:end].values, label='observed', linewidth=2.5)\n",
    "\n",
    "plt.plot(dft.groupby(['season', 'hour_of_week'])['observed'].mean().values, label='observed', linewidth=2.5)\n",
    "# plt.plot(reporting_pred.groupby(['season', 'hour_of_week'])['model'].mean().values, label='CT',color='red',  linewidth=2.5)\n",
    "plt.plot(dft.groupby(['season', 'hour_of_week'])['predicted'].mean().values, label='predicted', color='red', linewidth=4.5)\n",
    "\n",
    "# plt.plot(-dft['ghi'][start:end].values*10, label='ghi', color='orange', linestyle='--', linewidth=1, alpha=0.7)\n",
    "# plt.plot(dft['model'][start:end].values, label='oeem', color='green', linestyle='--', linewidth=1.5)\n",
    "# plt.plot(dft['new_model'][start:end].values, label='new_model', color='mediumseagreen',linestyle='--', linewidth=4.5)\n",
    "\n",
    "# plt.plot(dft['predicted'][start:end].values, label='new_model', color='red', linewidth=4.5)\n",
    "# plt.plot(dft['temperature'][start:end].values/20, label='temperature', color='mediumseagreen', linewidth=1, alpha=0.7)\n",
    "# make xtick and ytick labels larger\n",
    "#add two vertical line in x=168 and x=336\n",
    "plt.axvline(168, color='black', linewidth=1, linestyle='--' , alpha=0.5)\n",
    "plt.axvline(336, color='black', linewidth=1, linestyle='--' , alpha=0.5)\n",
    "plt.xticks(fontsize=16)\n",
    "#change s tick to time\n",
    "# plt.xticks(dft['start_local'].index[start:end].values, dft['start_local'].index[start:end].values, rotation=45)\n",
    "plt.yticks(fontsize=16)\n",
    "# transparent line on y=0\n",
    "plt.axhline(-0.02, color='black', linewidth=1, linestyle='--' , alpha=0.5)\n",
    "\n",
    "#make y axis the same for all plots\n",
    "plt.ylim(-2.7, 2)\n",
    "# plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "iqr = np.percentile(pred_baseline['observed'], 75) - np.percentile(pred_baseline['observed'], 25)\n",
    "mn = np.mean(pred_baseline['observed'])\n",
    "\n",
    "# get metrics both for model and predicted based on observed\n",
    "metrics = {}\n",
    "metrics['PNRMSE'] = PNRMSE(dft['observed'], dft['model'], iqr)\n",
    "metrics['PNRMSE_predicted'] = PNRMSE(dft['observed'], dft['predicted'], iqr)\n",
    "metrics['CVRMSE'] = CVRMSE(dft['observed'], dft['model'], mn)\n",
    "metrics['CVRMSE_predicted'] = CVRMSE(dft['observed'], dft['predicted'], mn)\n",
    "metrics['MBE'] = MBE(dft['observed'], dft['model'])\n",
    "metrics['MBE_predicted'] = MBE(dft['observed'], dft['predicted'])\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dft = reporting_pred.copy()\n",
    "dft = pred_baseline.copy()\n",
    "start = 24*0\n",
    "pltwindow = -1\n",
    "end = start + pltwindow\n",
    "# get the rows between start and end\n",
    "dft = dft.iloc[start:end]\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "# higher dpi for better quality\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "#x axis as hours\n",
    "# plt.plot(dft['start_local'][start:end], label='start_local', linestyle='--')\n",
    "plt.plot(dft['observed'].values, label='observed', linewidth=2.5)\n",
    "\n",
    "\n",
    "# plt.plot(-dft['ghi'].values*2, label='ghi', color='orange', linestyle='--', linewidth=1, alpha=0.7)\n",
    "# plt.plot(dft['model'].values, label='oeem', color='green', linestyle='--', linewidth=1.5)\n",
    "# plt.plot(dft['predicted'].values, label='new_model', color='red', linewidth=4.5)\n",
    "\n",
    "# plt.plot(dft['predicted'][start:end].values, label='new_model', color='red', linewidth=4.5)\n",
    "# plt.plot(dft['temperature'][start:end].values/20, label='temperature', color='mediumseagreen', linewidth=1, alpha=0.7)\n",
    "# make xtick and ytick labels larger\n",
    "#add two vertical line in x=168 and x=336\n",
    "# plt.axvline(23, color='black', linewidth=1, linestyle='--' , alpha=0.5)\n",
    "# plt.axvline(336, color='black', linewidth=1, linestyle='--' , alpha=0.5)\n",
    "# plt.xticks(fontsize=16)\n",
    "#change xtick to 1 to 48 and show every other 6 hours\n",
    "# plt.xticks([0, 11, 23, 35, 47], [1, 12, 24, 36, 48])\n",
    "\n",
    "# add the first and the last date to the xticks\n",
    "first_date = dft.index[0].date()\n",
    "last_date = dft.index[-1].date()\n",
    "plt.xticks([0, len(dft['observed'])], [first_date, last_date])\n",
    "\n",
    "# plt.yticks(fontsize=16)\n",
    "# transparent line on y=0\n",
    "plt.axhline(dft['observed'].mean(), color='black', linewidth=1, linestyle='--' , alpha=0.5)\n",
    "\n",
    "#make y axis the same for all plots\n",
    "# plt.ylim(-2.9, 2.9)\n",
    "# plt.xlim(0, 47)\n",
    "# plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dft = pred_baseline.copy()\n",
    "# dft['observed'].describe()\n",
    "# idx = dft['observed'].argmin() -24*2\n",
    "# print(idx)\n",
    "\n",
    "# selected_date = dft.index[idx].date()\n",
    "# # get the data for the selected date\n",
    "# selected_data = dft[dft.index.date == selected_date]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 24*0\n",
    "pltwindow = -1\n",
    "end = start + pltwindow\n",
    "dft = selected_data.copy()\n",
    "dft = repo\n",
    "# get the rows between start and end\n",
    "dft = dft.iloc[start:end]\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "# higher dpi for better quality\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "#x axis as hours\n",
    "# plt.plot(dft['start_local'][start:end], label='start_local', linestyle='--')\n",
    "plt.plot(dft['observed'].values, label='observed', linewidth=2.5)\n",
    "\n",
    "#change xtick to 1 to 48 and show every other 6 hours\n",
    "first_date = dft.index[0].date()\n",
    "last_date = dft.index[-1].date()\n",
    "plt.xticks([0, len(dft['observed'])], [first_date, last_date])\n",
    "\n",
    "\n",
    "plt.axhline(dft['observed'].mean(), color='black', linewidth=1, linestyle='--' , alpha=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "634.4/208"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_meter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reporting_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft = reporting_pred.copy()\n",
    "# dft = pred_baseline.copy()\n",
    "start = 24*0\n",
    "pltwindow = -1\n",
    "end = start + pltwindow\n",
    "# get the rows between start and end\n",
    "dft = dft.iloc[start:end]\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "# higher dpi for better quality\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "#x axis as hours\n",
    "# plt.plot(dft['start_local'][start:end], label='start_local', linestyle='--')\n",
    "plt.plot(dft['observed'].values, label='observed', linewidth=2.5)\n",
    "\n",
    "\n",
    "# plt.plot(-dft['ghi'].values*2, label='ghi', color='orange', linestyle='--', linewidth=1, alpha=0.7)\n",
    "# plt.plot(dft['model'].values, label='oeem', color='green', linestyle='--', linewidth=1.5)\n",
    "# plt.plot(dft['predicted'].values, label='new_model', color='red', linewidth=4.5)\n",
    "\n",
    "# plt.plot(dft['predicted'][start:end].values, label='new_model', color='red', linewidth=4.5)\n",
    "# plt.plot(dft['temperature'][start:end].values/20, label='temperature', color='mediumseagreen', linewidth=1, alpha=0.7)\n",
    "# make xtick and ytick labels larger\n",
    "#add two vertical line in x=168 and x=336\n",
    "# plt.axvline(23, color='black', linewidth=1, linestyle='--' , alpha=0.5)\n",
    "# plt.axvline(336, color='black', linewidth=1, linestyle='--' , alpha=0.5)\n",
    "# plt.xticks(fontsize=16)\n",
    "#change xtick to 1 to 48 and show every other 6 hours\n",
    "first_date = dft.index[0].date()\n",
    "last_date = dft.index[-1].date()\n",
    "plt.xticks([0, len(dft['observed'])], [first_date, last_date])\n",
    "\n",
    "\n",
    "# plt.yticks(fontsize=16)\n",
    "# transparent line on y=0\n",
    "plt.axhline(0, color='black', linewidth=1, linestyle='--' , alpha=0.5)\n",
    "\n",
    "#make y axis the same for all plots\n",
    "# plt.ylim(-2.9, 2.9)\n",
    "# plt.xlim(0, 47)\n",
    "# plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reporting_pred.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select reporting period after the intervention\n",
    "selected = reporting_pred.loc[reporting_pred.index >= PV_interventation]\n",
    "selected.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
