{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from eemeter.development.model import HourlyOptData, HourlyOptModel\n",
    "import pickle\n",
    "import multiprocessing as mp\n",
    "\n",
    "# from hourly_test_utils import *\n",
    "from applied_data_science.bigquery.data import Meter_Data\n",
    "from eemeter import eemeter as em\n",
    "from eemeter.common.metrics import BaselineTestingMetrics as Metrics\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load subsamples from the main MCE dataset\n",
    "dataset = 'mce_3_yr_precovid'\n",
    "subsample = 1\n",
    "has_solar = True\n",
    "\n",
    "cache_dir = Path(\"/app/.recurve_cache/data\").resolve()\n",
    "\n",
    "if 'data' in globals():\n",
    "    del data\n",
    "\n",
    "data = Meter_Data(dataset, subsample, \"hourly\", solar=has_solar, cache_dir=cache_dir)\n",
    "\n",
    "meta = data.df['meta']\n",
    "subsample_df = data.df['meter']\n",
    "ids = subsample_df.index.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_settings = em.HourlySettings(\n",
    "    TRAIN_FEATURES=['ghi'],\n",
    "    SUPPLEMENTAL_DATA= {'TS_SUPPLEMENTAL': None,\n",
    "                        'CATEGORICAL_SUPPLEMENTAL': {'PV_INSTALLATION_DATE': \"2019-05-14\"},},\n",
    ")\n",
    "\n",
    "alpha_opt = 0.15\n",
    "l1_ratio_opt = 0.05\n",
    "model_settings = em.HourlySettings(\n",
    "    ALPHA=alpha_opt,\n",
    "    L1_RATIO=l1_ratio_opt,\n",
    "    SELECTION=\"cyclic\",\n",
    "    MAX_ITER=1000,\n",
    "    SEED=42\n",
    ")\n",
    "months = [\n",
    "    \"jan\",\n",
    "    \"feb\",\n",
    "    \"mar\",\n",
    "    \"apr\",\n",
    "    \"may\",\n",
    "    \"jun\",\n",
    "    \"jul\",\n",
    "    \"aug\",\n",
    "    \"sep\",\n",
    "    \"oct\",\n",
    "    \"nov\",\n",
    "    \"dec\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting features for each fold of individual id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _each_month_data_dec(arglst):\n",
    "    non_neccessary_columns = ['jan_train', 'jan_test',\n",
    "       'feb_train', 'feb_test', 'mar_train', 'mar_test', 'apr_train',\n",
    "       'apr_test', 'may_train', 'may_test', 'jun_train', 'jun_test',\n",
    "       'jul_train', 'jul_test', 'aug_train', 'aug_test', 'sep_train',\n",
    "       'sep_test', 'oct_train', 'oct_test', 'nov_train', 'nov_test',\n",
    "       'dec_train', 'dec_test']\n",
    "    X_train, y_train, X_test, y_test = [], [], [], []\n",
    "    df, month, kwargs = arglst\n",
    "    idx_train = np.argwhere(np.isfinite(df[f\"{month}_train\"].values)).flatten()\n",
    "\n",
    "    idx_finite = np.argwhere(\n",
    "        np.isfinite(df.iloc[idx_train][\"observed\"].values)\n",
    "    ).flatten()\n",
    "    if len(idx_finite) == 0:\n",
    "        pass\n",
    "\n",
    "    idx_train = idx_train[idx_finite]\n",
    "\n",
    "    idx_test = np.argwhere(np.isfinite(df[f\"{month}_test\"].values)).flatten()\n",
    "\n",
    "    idx_finite = np.argwhere(\n",
    "        np.isfinite(df.iloc[idx_test][\"observed\"].values)\n",
    "    ).flatten()\n",
    "\n",
    "    if len(idx_finite) == 0:\n",
    "        pass\n",
    "\n",
    "    idx_test = idx_test[idx_finite]\n",
    "\n",
    "    df_train_temp = df.iloc[idx_train]\n",
    "    df_test_temp = df.iloc[idx_test]\n",
    "\n",
    "    df_train_temp['model'] = df_train_temp[f\"{month}_{'train'}\"]\n",
    "    df_test_temp['model'] = df_test_temp[f\"{month}_{'test'}\"]\n",
    "\n",
    "    df_train_temp = df_train_temp.drop(columns=non_neccessary_columns)\n",
    "    df_test_temp = df_test_temp.drop(columns=non_neccessary_columns)\n",
    "\n",
    "    train_data = HourlyOptData(df_train_temp, data_settings, **kwargs)\n",
    "    y_scaler = train_data._y_scaler\n",
    "\n",
    "    X_train, y_train = train_data.X, train_data.y\n",
    "\n",
    "    test_data = HourlyOptData(df_test_temp, data_settings, **kwargs)\n",
    "    X_test, y_test = test_data.X, test_data.y\n",
    "\n",
    "    return X_train, y_train, df_train_temp['model'].values, X_test, y_test, df_test_temp['model'].values, y_scaler\n",
    "\n",
    "def get_kfold_data_3_years(meta_id, df, kwargs):\n",
    "    arglst = [(df, month, kwargs) for month in months]\n",
    "    with mp.Pool(mp.cpu_count()) as pool:\n",
    "        res = pool.map(_each_month_data_dec, arglst)\n",
    "    return meta_id, res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickle a subsample features\n",
    "\n",
    "@Travis if you can use multiprocessing to speed up the process, that would be great. I tried mp for id level but for some reason it wasn't faster\n",
    "for me using mp worked better when I used it for each month level (3 times faster)\n",
    "\n",
    "\n",
    "Save the data with the following cell, and if you've already done it just skip it (I forgot to do that in my VM one time, which takes 57 minutes for one subsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample_features = []\n",
    "for meter_id in ids:\n",
    "    meter = subsample_df.loc[meter_id].copy()\n",
    "    meta_id = meta.loc[meter_id].copy()\n",
    "    unique_pairs = meta_id[['station_latitude', 'station_longitude']].drop_duplicates()\n",
    "    METADATA = {\n",
    "                'station_latitude': unique_pairs['station_latitude'].values[0],\n",
    "                'station_longitude': unique_pairs['station_longitude'].values[0],\n",
    "                    }\n",
    "    OUTPUT_FEATURES=['temperature', 'observed', 'model']\n",
    "    if 'ghi' in data_settings.TRAIN_FEATURES:\n",
    "            CONSIDER_SOLAR = True\n",
    "            OUTPUT_FEATURES.append('ghi')\n",
    "            OUTPUT_FEATURES.append('clearsky_ghi')\n",
    "    else:\n",
    "        CONSIDER_SOLAR = False\n",
    "    kwargs = {\n",
    "        'solar': CONSIDER_SOLAR,\n",
    "        'metadata': METADATA,\n",
    "        'outputs': OUTPUT_FEATURES,\n",
    "    }\n",
    "    res = get_kfold_data_3_years(meta_id, meter, kwargs)\n",
    "    subsample_features.append(res)\n",
    "\n",
    "subsample_results_saving_path = Path(\"/app/.recurve_cache/data/subsample_features\").resolve()\n",
    "subsample_results_saving_path.mkdir(exist_ok=True)\n",
    "import pickle\n",
    "with open(subsample_results_saving_path / f\"subsample_features_{dataset}_{subsample}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(subsample_features, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trining the models\n",
    "\n",
    "Read the subsample features here\n",
    "Also you can model them as well here!\n",
    "\n",
    "Have that in mind there is a missing_values_amount variable in HourlyOptData which shows if we exceed the 10% threshold during interpolation.\n",
    "Then if that's the case we won't have features at all and all of them would be None (like y_train = None)\n",
    "And if this is None then we skip it in the modeling. So you can count the number of \"OK\" ids if you want.\n",
    "\n",
    "Right now I'm taking the average of test PNRMSE for those who actually have values in the features, in the objective funciton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample_results_saving_path = Path(\"/app/.recurve_cache/data/subsample_features\").resolve()\n",
    "with open(subsample_results_saving_path / f\"subsample_features_{dataset}_{subsample}.pkl\", \"rb\") as f:\n",
    "    subsample_features = pickle.load(f)\n",
    "\n",
    "def id_model_dec(arglsts):\n",
    "    meter_id, kfold_features = arglsts\n",
    "    all_id_test_error = []\n",
    "    for i in range(len(kfold_features)):\n",
    "        X_train, y_train, oeem_train, X_test, y_test, oeem_test, y_scaler = kfold_features[i]\n",
    "        if y_train is None: \n",
    "            # print('bad id: ', meter_id.index.unique()[0])\n",
    "            break\n",
    "        model = HourlyOptModel(model_settings)\n",
    "        y_pred_train = model.fit_predict(X_train, y_train)\n",
    "        y_predict_train = y_scaler.inverse_transform(y_pred_train)\n",
    "        y_predict_train = y_predict_train.flatten()\n",
    "\n",
    "        y_train_inv = y_scaler.inverse_transform(y_train)\n",
    "        y_train_inv = y_train_inv.flatten()\n",
    "\n",
    "        y_pred_test = model.predict(X_test)\n",
    "        y_predict_test = y_scaler.inverse_transform(y_pred_test)\n",
    "        y_predict_test = y_predict_test.flatten()\n",
    "\n",
    "        y_test_inv = y_scaler.inverse_transform(y_test)\n",
    "        y_test_inv = y_test_inv.flatten()\n",
    "\n",
    "        iqr = np.percentile(y_train_inv, 75) - np.percentile(y_train_inv, 25)\n",
    "        rmse = np.sqrt(np.mean((y_predict_test - y_test_inv) ** 2))\n",
    "        test_pnrmse = rmse / iqr\n",
    "\n",
    "        all_id_test_error.append(test_pnrmse)\n",
    "    \n",
    "    id_test_pnrmse = np.mean(all_id_test_error)\n",
    "    # print(id_test_pnrmse)\n",
    "    return id_test_pnrmse\n",
    "\n",
    "# for i in range(len(subsample_features)):\n",
    "#     lst = subsample_features[i]\n",
    "#     print(i)\n",
    "#     print(id_model_dec(lst))\n",
    "\n",
    "with mp.Pool(mp.cpu_count()) as pool:\n",
    "    model_res = pool.map(id_model_dec, subsample_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_fcn(X, gradient=np.array([])):\n",
    "    timer_start = time.time()\n",
    "\n",
    "    alpha, l1_ratio = X\n",
    "\n",
    "    model_settings.ALPHA = alpha\n",
    "    model_settings.L1_RATIO = l1_ratio\n",
    "    \n",
    "    with mp.Pool(mp.cpu_count()-1) as pool:\n",
    "        model_res = pool.map(id_model_dec, subsample_features)\n",
    "\n",
    "    model_res = np.array(model_res)\n",
    "    #remove nan values\n",
    "    model_res = model_res[~np.isnan(model_res)]\n",
    "    \n",
    "    obj = np.mean(model_res)\n",
    "    print(f\"alpha: {alpha}, l1_ratio: {l1_ratio}, obj: {obj}\")\n",
    "    print(f\"Time taken: {time.time()-timer_start}\")\n",
    "    return float(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization settings\n",
    "import rbfopt\n",
    "\n",
    "opt_options = {\n",
    "    \"global\": {\"algorithm\": \"RBFOpt\", \n",
    "               \"stop_criteria_type\": 'Iteration Maximum', \n",
    "               \"stop_criteria_val\": 2, \n",
    "               \"initial_step\": 0.01, # percentage},\n",
    "               \"xtol_rel\": 1E-5,\n",
    "               \"ftol_rel\": 1E-5,\n",
    "               \"initial_pop_multiplier\": 2,\n",
    "    },\n",
    "}\n",
    "\n",
    "x0 = [0.01, 0.01]\n",
    "bnds = [(0.0001, 1), (0.0001, 1)]\n",
    "\n",
    "\n",
    "bnds = np.array(bnds).T\n",
    "n_dim = np.size(bnds[0])\n",
    "\n",
    "var_type = ['R']*n_dim  \n",
    "max_eval = 400\n",
    "max_time = 1E30\n",
    "rbfopt_initialize = True\n",
    "bb = rbfopt.RbfoptUserBlackBox(n_dim, np.array(bnds[0]), np.array(bnds[1]),\n",
    "                                np.array(var_type), obj_fcn)\n",
    "\n",
    "\n",
    "bonmin_path = \"/app/applied_data_science/tools/optimization/coin-or/bonmin\"\n",
    "ipopt_path = \"/app/applied_data_science/tools/optimization/coin-or/ipopt\"\n",
    "rbfsettings = rbfopt.RbfoptSettings(max_iterations=max_eval,\n",
    "                                    max_evaluations=max_eval,\n",
    "                                    max_cycles=1E30,\n",
    "                                    max_clock_time=max_time,\n",
    "                                    minlp_solver_path=bonmin_path, \n",
    "                                    nlp_solver_path=ipopt_path,)\n",
    "                                    \n",
    "algo = rbfopt.RbfoptAlgorithm(rbfsettings, bb, init_node_pos=x0, do_init_strategy=rbfopt_initialize)\n",
    "loss, x_opt, itercount, evalcount, fast_evalcount = algo.optimize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
